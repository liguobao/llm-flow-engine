# LLM Flow Engine

ä¸€ä¸ªåŸºäº DSLï¼ˆé¢†åŸŸç‰¹å®šè¯­è¨€ï¼‰çš„ LLM å·¥ä½œæµå¼•æ“ï¼Œæ”¯æŒå¤šæ¨¡å‹åä½œã€ä¾èµ–ç®¡ç†å’Œç»“æœæ±‡æ€»ã€‚

## âœ¨ æ ¸å¿ƒç‰¹æ€§

- ğŸ”¥ **DSL å·¥ä½œæµå®šä¹‰** - ä½¿ç”¨ YAML æ ¼å¼å®šä¹‰å¤æ‚çš„ LLM å·¥ä½œæµ
- ğŸš€ **DAG ä¾èµ–ç®¡ç†** - æ”¯æŒèŠ‚ç‚¹ä¾èµ–å…³ç³»å’Œå¹¶è¡Œæ‰§è¡Œ
- ğŸ”— **å ä½ç¬¦è§£æ** - ä½¿ç”¨ `${node.output}` è¯­æ³•å®ç°èŠ‚ç‚¹é—´æ•°æ®ä¼ é€’
- ğŸ¤– **å¤šæ¨¡å‹æ”¯æŒ** - æ”¯æŒä¸åŒ LLM æ¨¡å‹çš„è°ƒç”¨å’Œç»“æœæ±‡æ€»
- âš™ï¸ **çµæ´»é…ç½®** - è‡ªå®šä¹‰æ¨¡å‹é…ç½®å’Œå‚æ•°ç®¡ç†
- ğŸ”„ **å¼‚æ­¥æ‰§è¡Œ** - é«˜æ•ˆçš„å¼‚æ­¥ä»»åŠ¡å¤„ç†å’Œé”™è¯¯é‡è¯•

## ğŸ—ï¸ é¡¹ç›®ç»“æ„

```text
llm_flow_engine/
â”œâ”€â”€ __init__.py           # åŒ…åˆå§‹åŒ–
â”œâ”€â”€ flow_engine.py        # ä¸»å¼•æ“å…¥å£
â”œâ”€â”€ dsl_loader.py         # DSL è§£æå™¨
â”œâ”€â”€ dag_workflow.py       # DAG å·¥ä½œæµç®¡ç†
â”œâ”€â”€ executor.py           # ä»»åŠ¡æ‰§è¡Œå™¨
â”œâ”€â”€ builtin_functions.py  # å†…ç½®å‡½æ•°åº“
â”œâ”€â”€ model_config.py       # æ¨¡å‹é…ç½®ç®¡ç†
â””â”€â”€ utils.py             # å·¥å…·å‡½æ•°
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. å®‰è£…ä¾èµ–

```bash
pip install -r requirements.txt
```

### 2. è¿è¡Œæ¼”ç¤º

```bash
# å®Œæ•´åŠŸèƒ½æ¼”ç¤º
python demo_example.py

# åŸºç¡€æ¼”ç¤º
python demo.py
```

### 3. è‡ªå®šä¹‰å·¥ä½œæµ

åˆ›å»º YAML å·¥ä½œæµæ–‡ä»¶ï¼š

```yaml
metadata:
  version: "1.0"
  description: "å¤šæ¨¡å‹é—®ç­”æ±‡æ€»å·¥ä½œæµ"

input:
  type: "start"
  name: "workflow_input"
  data:
    question: "ç”¨æˆ·è¾“å…¥çš„åŸå§‹é—®é¢˜"

executors:
  - name: text_processing
    type: "task"
    func: text_process
    custom_vars:
      text: "${workflow_input.question}"

  - name: model1_answer
    type: "task"
    func: llm_simple_call
    custom_vars:
      user_input: "${text_processing.output}"
      model: "gemma3:1b"
    depends_on: ["text_processing"]

  - name: summary_step
    type: "task"
    func: llm_simple_call
    custom_vars:
      user_input: "è¯·æ±‡æ€»: ${model1_answer.output}"
      model: "gemma3:4b"
    depends_on: ["model1_answer"]

output:
  result: "${summary_step.output}"
```

## ğŸ“– API ä½¿ç”¨

```python
from llm_flow_engine import FlowEngine

# åˆ›å»ºå¼•æ“å®ä¾‹
engine = FlowEngine()

# ä» DSL æ–‡ä»¶æ‰§è¡Œå·¥ä½œæµ
result = await engine.run_workflow_from_dsl(
    "demo_qa.yaml", 
    question="ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ"
)

print(result)
```

## ğŸ”§ é…ç½®è¯´æ˜

### æ¨¡å‹é…ç½®

æ”¯æŒå¤šç§ LLM æ¨¡å‹ï¼š

- **Ollama æœ¬åœ°æ¨¡å‹**: gemma3:1b, gemma3:4b, qwen3:8b
- **OpenAI æ¨¡å‹**: gpt-3.5-turbo, gpt-4
- **å…¶ä»– API æ¨¡å‹**: å¯æ‰©å±•æ”¯æŒ

### å ä½ç¬¦è¯­æ³•

- `${workflow_input.key}` - å¼•ç”¨å·¥ä½œæµè¾“å…¥
- `${node_name.output}` - å¼•ç”¨èŠ‚ç‚¹è¾“å‡º
- æ”¯æŒåµŒå¥—å¯¹è±¡å’Œå¤æ‚æ•°æ®ç»“æ„

## ğŸ§ª æµ‹è¯•éªŒè¯

é¡¹ç›®åŒ…å«å®Œæ•´çš„æ¼”ç¤ºç¤ºä¾‹ï¼š

- âœ… å·¥ä½œæµæ‰§è¡ŒæˆåŠŸ
- âœ… å ä½ç¬¦è§£ææ­£å¸¸  
- âœ… èŠ‚ç‚¹ä¾èµ–å…³ç³»æ­£ç¡®å¤„ç†
- âœ… å¤šæ¨¡å‹å¹¶è¡Œ/ä¸²è¡Œæ‰§è¡Œ
- âœ… é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶

## ğŸ“ è®¸å¯è¯

MIT License

## ğŸ¤ è´¡çŒ®

æ¬¢è¿æäº¤ Issues å’Œ Pull Requestsï¼

---

â­ å¦‚æœè¿™ä¸ªé¡¹ç›®å¯¹ä½ æœ‰å¸®åŠ©ï¼Œè¯·ç»™ä¸ª Starï¼
