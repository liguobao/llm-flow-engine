#!/usr/bin/env python3
"""
LLM Flow Engine æ¼”ç¤ºç¤ºä¾‹
å±•ç¤ºæ–°æ¶æ„çš„æ ¸å¿ƒåŠŸèƒ½ï¼š
1. è‡ªå®šä¹‰æ¨¡å‹é…ç½®
2. DSLå·¥ä½œæµæ‰§è¡Œ
3. å¤šæ¨¡å‹é—®ç­”æ±‡æ€»
"""
import asyncio
import aiohttp
from loguru import logger
from llm_flow_engine import FlowEngine, ModelConfigProvider

async def demo_basic_usage():
    logger.info("ğŸš€ LLM Flow Engine æ¼”ç¤º")
    logger.info("=" * 50)
    
    # 1. è‡ªå®šä¹‰æ¨¡å‹é…ç½®
    logger.info("\nâš™ï¸  1. è‡ªå®šä¹‰æ¨¡å‹é…ç½®")
    
    ollama_host = "http://192.168.50.57:11434"
    custom_models =await load_models_from_ollama(ollama_host)
    # åˆ›å»ºè‡ªå®šä¹‰é…ç½®æä¾›è€…
    custom_provider = ModelConfigProvider(custom_models)
    engine = FlowEngine(custom_provider)
    # æŸ¥çœ‹æ”¯æŒçš„æ¨¡å‹
    models = engine.model_provider.list_supported_models()
    total_models = sum(len(model_list) for model_list in models.values())
    logger.info(f"æ”¯æŒ {total_models} ä¸ªæ¨¡å‹ï¼Œæ¶µç›– {len(models)} ä¸ªå¹³å°")

    # 2. ä»æœ¬åœ°æ–‡ä»¶è¯»å–DSLå¹¶æ‰§è¡Œå¤šæ¨¡å‹é—®ç­”æ±‡æ€»
    logger.info("\nğŸ¤– 2. æœ¬åœ°Ollamaæ¨¡å‹é—®ç­”æ±‡æ€»æ¼”ç¤º")
    logger.info("é—®é¢˜: ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ")
    logger.info("æ¨¡å‹: gemma3:1b, qwen2.5:0.5b, deepseek-r1:1.5b")
    logger.info("æ–¹æ¡ˆ: ä¸‰ä¸ªå°æ¨¡å‹åˆ†åˆ«å›ç­”ï¼Œç„¶åç”¨gemma3:4bæ±‡æ€»åˆ†æ")
    
    try:
        # è¯»å–æœ¬åœ°DSLæ–‡ä»¶
        with open('./examples/demo_qa.yaml', 'r', encoding='utf-8') as f:
            dsl_content = f.read()

        logger.info("âœ… æˆåŠŸè¯»å–DSLæ–‡ä»¶: demo_qa.yaml")
        logger.info("ğŸ”„ å¼€å§‹è°ƒç”¨æœ¬åœ°Ollamaæ¨¡å‹...")

        # å‡†å¤‡å·¥ä½œæµè¾“å…¥å‚æ•°
        workflow_input = {
            "question": "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ"  # ç”¨æˆ·å®é™…çš„é—®é¢˜
        }
        
        # æ‰§è¡Œå¤šæ¨¡å‹é—®ç­”DSL
        qa_result = await engine.execute_dsl(dsl_content, inputs={"workflow_input": workflow_input})
        
        if qa_result['success']:
            print("âœ… å¤šæ¨¡å‹é—®ç­”æ‰§è¡ŒæˆåŠŸ:")
            
            # æ˜¾ç¤ºå·¥ä½œæµå…ƒæ•°æ®
            if 'metadata' in qa_result:
                metadata = qa_result['metadata']
                logger.info(f"\nğŸ“‹ å·¥ä½œæµä¿¡æ¯:")
                logger.info(f"   ç‰ˆæœ¬: {metadata.get('version', 'æœªæŒ‡å®š')}")
                logger.info(f"   æè¿°: {metadata.get('description', 'æœªæŒ‡å®š')}")

            # æ˜¾ç¤ºå„ä¸ªæ¨¡å‹çš„å›ç­”
            model_answers = []
            for name, exec_result in qa_result['results'].items():
                if exec_result.status == 'success':
                    if name.endswith('_answer'):
                        model_name = name.replace('_answer', '').replace('model', 'Model')
                        answer = exec_result.output
                        logger.info(f"\nğŸ“ {model_name} å›ç­”:")
                        # æˆªå–å›ç­”çš„å‰200ä¸ªå­—ç¬¦ç”¨äºæ˜¾ç¤º
                        display_answer = answer[:200] + "..." if len(answer) > 200 else answer
                        logger.info(f"   {display_answer}")
                        model_answers.append(answer)
                    elif name == 'summary_step':
                        logger.info(f"\nğŸ¯ gemma3:4b æ±‡æ€»åˆ†æ:")
                        summary_answer = exec_result.output
                        # æˆªå–æ±‡æ€»çš„å‰300ä¸ªå­—ç¬¦ç”¨äºæ˜¾ç¤º
                        display_summary = summary_answer[:300] + "..." if len(summary_answer) > 300 else summary_answer
                        logger.info(f"   {display_summary}")
                else:
                    logger.error(f"  âŒ {name}: {exec_result.error}")

            # æ˜¾ç¤ºå·¥ä½œæµè¾“å‡ºä¿¡æ¯
            if 'output' in qa_result:
                logger.info(f"\nğŸ“¤ å·¥ä½œæµè¾“å‡º:")
                for key, value in qa_result['output'].items():
                    logger.info(f"   {key}: {value[:100]}..." if len(str(value)) > 100 else f"   {key}: {value}")

            logger.info(f"\nğŸ“Š æ‰§è¡Œç»Ÿè®¡: {len(model_answers)} ä¸ªæ¨¡å‹æˆåŠŸå›ç­”ï¼Œæœ€åå®Œæˆæ±‡æ€»")

        else:
            logger.error(f"âŒ å¤šæ¨¡å‹é—®ç­”å¤±è´¥: {qa_result['error']}")

    except FileNotFoundError:
        logger.error("âŒ æœªæ‰¾åˆ°DSLæ–‡ä»¶: demo_qa.yaml")
    except Exception as e:
        logger.error(f"âŒ DSLæ‰§è¡Œå‡ºé”™: {str(e)}")

    logger.info("\nğŸ‰ æ¼”ç¤ºå®Œæˆï¼")
    logger.info("æ ¸å¿ƒç‰¹æ€§å±•ç¤º:")
    logger.info("âœ… è‡ªå®šä¹‰æ¨¡å‹é…ç½®ç®¡ç†")
    logger.info("âœ… æœ¬åœ°DSLæ–‡ä»¶è¯»å–æ‰§è¡Œ")
    logger.info("âœ… å¤šæ¨¡å‹å¹¶è¡Œ/ä¸²è¡Œæ‰§è¡Œ")
    logger.info("âœ… å¤æ‚ä¾èµ–å…³ç³»å¤„ç†")
    logger.info("âœ… æ¨¡æ‹ŸLLMè°ƒç”¨å’Œç»“æœæ±‡æ€»")

async def load_models_from_ollama(ollama_host):
    async with aiohttp.ClientSession() as session:
        async with session.get(f"{ollama_host}/v1/models") as resp:
            resp = await resp.json()
            model_items= resp.get("data",[])
    ollama_model_config = {
            'platform': 'ollama', 
            'api_url': f'{ollama_host}/api/chat',
            'auth_header': None,
            'message_format': 'ollama',
            'max_tokens': 2048,  # é€‚åˆ1Bæ¨¡å‹çš„tokené™åˆ¶
            'supports': ['temperature', 'top_k', 'top_p']
    }
    
    custom_models = {}
    for model_item in model_items:
        model_name = model_item["id"]
        custom_models[model_name] = ollama_model_config
    return custom_models

if __name__ == '__main__':
    asyncio.run(demo_basic_usage())
